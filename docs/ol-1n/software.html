<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project 0L-1N | Software</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <nav>
        <a href="index.html">Main</a>
        <a href="mechanical.html">Mechanical</a>
        <a href="electrical.html">Electrical</a>
        <a href="software.html" class="active">Software</a>
        <a href="integration.html">Integration</a>
        <a href="reflection.html">Process & Budget</a>
    </nav>

    <div class="container">
        <h1>Software & Firmware Setup (Dependencies and Libraries)</h1>
        
        <div style="text-align:center; margin-bottom: 30px;">
            <a href="https://github.com/Angad-C/PIE-Final-Project-Code/tree/main" class="btn">View Full Repository on GitHub</a>
        </div>

        <section>
            <h2>For the Body Raspberry Pi</h2>
            <p>The setup process begins by flashing a 32GB microSD card with Raspberry Pi OS (Linux) using the Raspberry Pi Imager. While the Imager offers options to pre-configure SSH and Wi-Fi, I found that the automatic network connection failed during the initial boot. Consequently, I completed the setup manually by inserting the SD card into the Pi and connecting a monitor, USB mouse, and keyboard. Once the desktop GUI loaded, I manually connected the Pi to the same Wi-Fi network as my computer to enable remote programming via SSH. I also recommend pairing the Bluetooth controller via the computer Bluetooth menu at this stage, as it is often faster than using the command line later.</p>
            <p>After the OS was configured, I established a virtual environment (venv) to isolate the project dependencies. For motor control, I selected the pigpio library over the standard RPi.GPIO library, as the latter failed to generate the necessary waveforms on the Raspberry Pi.</p>
            <p>This can be done with the following commands:</p>
            <pre>
Create the virtual environment to hold your project libraries: python3 -m venv myenv
Activate the environment (you must do this every time you open a new terminal): source myenv/bin/activate
            </pre>
            <p>The pigpio library requires a system-level daemon to access the hardware pins. Return to the home directory and install it via apt:</p>
            <pre>cd ~ sudo apt update sudo apt install pigpio</pre>
            <p>You must run the daemon before your Python script can control the motors:</p>
            <pre>sudo pigpiod (Note: If the command above does not work, try: sudo systemctl start pigpiod)</pre>
            <p>With the virtual environment active, install the evdev library for handling Bluetooth controller inputs:</p>
            <pre>pip install evdev</pre>
            <p>If you did not pair the controller via the GUI, use the bluetoothctl tool:</p>
            <pre>bluetoothctl</pre>
            <p>Inside the bluetooth interface, run: scan on (Wait until your controller appears in the list, then copy its MAC address/Serial Number) scan off</p>
            <p>Run the connection commands using your specific Serial Number: pair &lt;serial number&gt; connect &lt;serial number&gt; trust &lt;serial number&gt; (The 'trust' command ensures it reconnects automatically in the future)</p>
            <p>Now you can run raspi_body.py on the body Raspberry Pi.</p>
        </section>

        <section>
            <h2>For the Head Raspberry Pi</h2>
            <p>Similar to the Body Raspberry Pi, the Head Raspberry Pi requires a fresh OS installation and a dedicated Python virtual environment to manage the complex computer vision libraries. Follow the same steps outlined above to flash the OS, configure Wi-Fi/SSH, and create your virtual environment (venv).</p>
            
            <h3>Camera Module</h3>
            <p>Before the software can access the video feed, the camera hardware interface must be enabled at the kernel level.</p>
            <pre>
Open the system configuration tool: sudo raspi-config
Navigate to Interface Options â†’ Legacy Camera.
Select Enable and exit the tool.
Reboot the Raspberry Pi for these changes to take effect.
            </pre>
            <p>Dependency Installation This can be done with the following commands:</p>
            <p>OpenCV is a heavy library that requires several compiled dependencies for image processing and GUI support. Update your package manager and install the build tools:</p>
            <pre>sudo apt update sudo apt upgrade sudo apt install build-essential cmake git libgtk-3-dev libavcodec-dev libavformat-dev libswscale-dev</pre>
            <p>With your virtual environment active (source myenv/bin/activate), install the libraries required to interface with the camera hardware. We use picamera with the array submodule to capture raw data frames:</p>
            <pre>pip install "picamera[array]"</pre>
            <p>Next, install the core computer vision processing libraries. We utilize opencv-python for image manipulation and ultralytics to implement YOLO (You Only Look Once) object detection models:</p>
            <pre>pip install opencv-python pip install ultralytics</pre>
            <p>For the robot's audio output, install pygame, which handles MP3 playback:</p>
            <pre>pip install pygame</pre>
            <p>Finally, download or record an MP3 file and save it in the same folder as the code to serve as the robot's interaction sound. Now you can run raspi_head.py on the head Raspberry Pi.</p>
        </section>

        <section>
            <h2>For the Adafruit Feather nRF52840 Express (Remote Controller)</h2>
            <p>The firmware for the remote controller converts an Adafruit Feather nRF52840 Express into a Bluetooth-based controller. It reads analog signals from two joysticks, each of which control one of the motors (tank-drive control) and digital signals from a rotary encoder (robot head control), processes them, and transmits them via bluetooth to the Raspberry Pi (which is connected to the robot) inside the robot, enabling the user to wirelessly control the motors.</p>
            <p>We programmed the controller in C++ using the Arduino IDE. The specific Board Support Packages for the nRF52840 must be installed manually, since they are not included in the default Arduino installation. Here is how we set the editor up:</p>
            <ul>
                <li>Install the latest version of the Arduino IDE</li>
                <li>Install Adafruit nRF52 Board Support Package:</li>
                <li>Open Arduino IDE and go to File > Preferences (macOS: Arduino IDE > Settings).</li>
                <li>Locate the "Additional Boards Manager URLs" field.</li>
                <li>Paste the following URL into the box (comma-separated if others exist): https://adafruit.github.io/arduino-board-index/package_adafruit_index.json Click OK.</li>
                <li>Go to Tools > Board > Boards Manager</li>
                <li>Search for "Adafruit nRF52".</li>
                <li>Click Install on the package named "Adafruit nRF52 by Adafruit".</li>
                <li>Install Required Libraries:</li>
                <li>Go to Sketch > Include Library > Manage Libraries...</li>
                <li>Search for "Adafruit Bluefruit nRF52".</li>
                <li>Install the library named "Adafruit Bluefruit nRF52 Libraries" (ensure that it is by Adafruit).</li>
            </ul>
            <p>Now that our editor was set up, we could run our code to interpret the signals from the joysticks and the rotary encoder and transmit them over bluetooth. After writing our code, we ran it by:</p>
            <p>Connect the nRF52840 to the PC via USB. In Arduino IDE, select Tools > Board > Adafruit Feather nRF52840 Express. Click Upload (Arrow Icon). Once uploaded, open the Bluetooth settings on the target device (e.g., Raspberry Pi) and scan. Pair with "My Robot Controller". The device will now be recognized as a standard Gamepad and the Raspberry Pi should be able to receive data over bluetooth from the controller.</p>
            <p>Note that the final 3 steps are to be completed only when the Raspberry Pi setup above is complete. The Raspberry Pi code must be running for it to detect the controller. Now you can run Controller_Code.ino on the Adafruit Feather. Note that you will probably be asked to make a folder for the code file to be in before it runs, this is normal and is just how .ino files work; make the folder and run the code.</p>
        </section>

        <section>
            <h2>Description of the code logic. How does it interpret sensor data? How does the controller work?</h2>
            
            <h3>For the Head Raspberry Pi</h3>
            <p>raspi_head.py First it initializes the camera and the pygame audio player. It then imports or downloads the YOLO v8 model weights used to run an inference model. Inside the code there is a function called playaudio which is responsible for playing a sound. Here only one specific sound is used which is the r2d2 beep sound.</p>
            <p>There is also a while loop that takes a photo with the camera and runs a YOLO inference on it. The YOLO algorithm detects objects in the picture, like a person, cat, dog, etc. The algorithm checks if the algorithm detects a person and if it does it calls the playaudio function which plays the sound. The loop repeats every 2s because the YOLO inference takes 2s to run on the raspberry pi.</p>

            <h3>For the Body Raspberry Pi</h3>
            <p>raspi_body.py First it imports dependencies for bluetooth communication (evdev), motor control (pigpio) and delays (time) first it checks if pigpio is active as pigpio must be turned on via the command sudo pigpio before running the program. After this it defines and initializes the pins on the raspberry pi that control the 3 motors 22, 23 and 24. After this it sets up the waveform to transmit through the 3 pins. We are using the sabertooth motor controller which requires a very specific waveform which is 50Hz and contains high pulses that are 1 - 2 ms long. The waveform should look like: 20ms low followed by 1-2ms high and that repeats. The speed of the motor is encoded in the length of the high pulses. If the pulse is 1ms long it is full speed reverse. If it is 2ms long it is full speed forward. If it is 1.5 ms long it will be still. The motors are commanded to be still on startup and the frequency of the waveform is set as 50Hz initially and not changed in the rest of the code.</p>
            <footer>Project Report | 0L-1N</footer>